\section{Why is technology not neutral?}

We know in sociology that power imbalances manifest as unequal distribution of power and privileges. These asymmetrical power dynamics grant certain groups more control, influence, and social advantages, thus enabling biases, inequalities, and discriminatory practices. The advent of AI poses a potential risk of perpetuating those inequalities and biases and even generating new prejudices and forms of discriminations. While AI is often regarded as a neutral tool, it is essential to acknowledge that the relationship between humans and technologies is far from being neutral.

To understand why technology and AI technologies cannot be considered neutral, we shall take into account two concepts: 
\begin{enumerate}
    \item the idea of “duality of technology” 
    \item the idea of “politics of technology”
\end{enumerate}

\textbf{Orlikowski}’s influential work in 1992 introduced the concept of duality of technology to illustrate how technology is both physically constructed within a social context and socially constructed through the meanings attributed to it by various actors. The duality of technology principle illustrated:
\begin{itemize}
    \item \textbf{Technology is the product of human actions}: this means that technology is an outcome of human actions such as design, development, appropriation, and modification.
    \item \textbf{Technology is the medium of human actions}: technology facilitates and constrains human actions through the provision of interpretative schemes, facilities, and norms.
    \item \textbf{Institutional conditions of interactions with technology}: institutional properties influence humans in the interaction with technology, for instance: intentions, professional norms, state of the art in material and knowledge, design standard, and available resources in terms of time, money, and skills.
    \item \textbf{Institutional consequences of interaction with technology}: interaction with technology influences the institutional properties of an organization through enforcing or transforming structure of signification, dominance, and legitimisation.
\end{itemize}

In brief, according to the duality of technology principle, human actors use technology to constitute structure, but at the same time, technology becomes part of the social structure constraining individual actors. This means that we need to recognize that technology is not just a tool, but it is a social actor that shapes and is shaped by humans’ actions.

The political intakes of technology and artifacts are elaborated by \textbf{Langdon Winner} in two ways in which technological artifacts contain political properties:
\begin{enumerate}
    \item They denote a certain kind of arrangements of power and authority within society. Firstly, the design or specific properties of artifacts may cater to certain kind of social interests while excluding the others. Winner refers to artifacts in which the very process of technical development is so thoroughly biased in a particular direction that it regularly produces results counted as wonderful breakthroughs by some social interests and crushing setbacks by others.
    \item Secondly, technological systems may encompass more rigid forms of politics whereby they either require or are compatible with certain kind of power arrangements. Such technologies are often called "inherently political".
\end{enumerate}

The concept of “duality of technology” and the idea of “politics of technology” reminds us of the transformative effect of social factors on AI.

\section{Why does AI discriminate?}
Bias can infect Artificial Intelligence technology at several entry points. \textbf{Kleinberg} identifies risks of discrimination in the choice of outcome, the selection of input information used to train it, and the training procedure, including the training data.

\begin{itemize}
    \item \textbf{Choice of outcome}: Decisions regarding what outcome to predict or how to weight different outcomes together.
    \item \textbf{Input information}: Decisions about what candidate predictors to collect, construct, and provide to the training algorithm for possible inclusion in the final statistical model.
    \item \textbf{Training procedure}: The training procedure, based on past data, to produce the screener. \newline
    \textbf{Barocas and Selbst} propose a different taxonomy of the sources of biases:
    \begin{itemize}
        \item \textbf{Target variables}: Subjectivity in defining target variables may lead to disproportionate adverse effects on certain protected groups.
        \item \textbf{Training data}: Features of training data may result in discrimination in AI decision-making as they shape the model, which, in turn, generates certain outcomes.
        \item \textbf{Relevant features}: The process of determining categories of data that will be considered or weighted for decision-making.
        \item \textbf{Proxy discrimination}: Criteria may inadvertently act as proxies for membership in a protected class, causing indirect discrimination against people from those protected classes.
        \item \textbf{Intentional discrimination by programmers}: Deliberate discriminatory actions or choices made by programmers or developers.
    \end{itemize}
\end{itemize}

\section{What is discrimination in legal theory?}
The current understanding of antidiscrimination law doesn’t always effectively address the challenges posed by algorithmic discrimination. \newline
Discrimination, from a legal perspective, is generally understood as a violation of the principle of equal treatment. However, there are certain considerations about how the antidiscrimination principle operates that highlight the shortcomings in addressing discriminations arising from Artificial Intelligence.

\begin{itemize}
    \item \textbf{Allocative v. representative harm}:
    \begin{itemize}
        \item \textbf{Allocative Harm}: Unequal allocation or distribution of resources, benefits, or opportunities that disadvantage a particular group based on their protected characteristics.
        \item \textbf{Representative Harm}: Negative impact due to under-representation or misrepresentation of certain groups in various contexts.
    \end{itemize}
    
    \item \textbf{Direct v. indirect discrimination}:
    \begin{itemize}
        \item \textbf{Direct Discrimination}: Treating a person or group less favorably based on protected characteristics.
        \item \textbf{Indirect Discrimination}: Occurs when a seemingly neutral policy disproportionately affects a specific group with protected characteristics.
    \end{itemize}
    
    \item \textbf{Impact v. intent}:
    \begin{itemize}
        \item \textbf{Impact (Disparate Impact)}: Focuses on the consequences or effects of a policy, practice, or decision on a particular group.
        \item \textbf{Intent (Disparate Treatment)}: Considers deliberate or intentional acts of discrimination based on protected characteristics.
    \end{itemize}
    
    \item \textbf{Intersectionality}:
    \begin{itemize}
        \item \textbf{Multiple Identities}: Recognizes that individuals have multiple intersecting identities (race, gender, age, sexual orientation, disability, etc.) influencing their experiences and vulnerabilities.
        \item \textbf{Unique Experiences}: Individuals with intersecting identities may face unique forms of discrimination and disadvantages that cannot be fully understood by examining each identity in isolation.
    \end{itemize}
\end{itemize}

\section{Why could the principle of nondiscrimination fail in confronting algorithmic biases?}

Given the theoretical framework of antidiscrimination legal theory, I will now highlight four problems that emerge when confronting algorithmic biases with this mainstream understanding of antidiscrimination law:

\begin{itemize}
    \item \textbf{Representative harms}
    
    From a discrimination theory perspective, biases in AI applications can be classified into two types of harms: allocative harm and representative harm.
    
    \begin{itemize}
        \item \textbf{Allocative harm} implies the unfair distribution of resources among individuals based on a specific factor (e.g., gender in the aforementioned Amazon case), withholding opportunities or resources from certain individuals.
        
        \item \textbf{Representative harm} occurs when systems reinforce the subordination of a group along identity lines.
    \end{itemize}
    
    \item \textbf{Direct v. indirect discrimination}
    
    The conceptual legal tools of direct and indirect discrimination face shortcomings when confronted with AI biases.
    
    \begin{itemize}
        \item \textbf{Direct discrimination} involves treating one person less favorably than another in a comparable situation.
        
        \item \textbf{Indirect discrimination} arises from apparently neutral practices or rules that put members of protected categories at a particular disadvantage. Challenges include proving algorithmic indirect discrimination and exceptions that justify the use of AI based on its predictive capacity and efficiency.
    \end{itemize}
    
    \item \textbf{Proxy discrimination}
    
    Proxy discrimination in algorithms occurs when an AI system uses proxy variables correlated with protected characteristics (e.g., race, gender, or age) to indirectly discriminate against certain groups. The system might not explicitly use the protected characteristic as an input but relies on closely correlated proxies to discriminate indirectly.
    
    \item \textbf{Data meaning and intersectionality}
    
    The problem here involves the meaning of categories as algorithmic inputs and intersectionality discrimination. For instance, false assumptions about sex, gender, and sexuality being binary, static, or concordant can be perpetuated, amplified, and justified by AI. ML algorithms can reproduce and amplify prejudice, stereotypes, and inequalities based on misconceptions about sex and gender in datasets.
\end{itemize}

